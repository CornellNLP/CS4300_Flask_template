{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the imports used in A2\n",
    "import re\n",
    "# import json\n",
    "from glob import glob\n",
    "import os\n",
    "from io import StringIO\n",
    "from itertools import groupby\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import bs4\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# Imports that might help with various functionality\n",
    "import functools\n",
    "import operator\n",
    "\n",
    "# Additional imports from A3\n",
    "from __future__ import print_function\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import Levenshtein  # package python-Levenshtein\n",
    "\n",
    "# Additional imports from A5\n",
    "from __future__ import print_function\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import json\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Ensure that your kernel is using Python3\n",
    "assert sys.version_info.major == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1378 women\n",
      "Each woman's entry is a dictionary with the following keys...\n",
      "dict_keys(['summary', 'name'])\n"
     ]
    }
   ],
   "source": [
    "# import the json\n",
    "import json\n",
    "with open(\"updated_data.json\", \"r\") as f:\n",
    "    women_summaries = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a list of women to keep as a global variable\n",
    "women_names = list()\n",
    "for i in range(len(women_summaries)):\n",
    "    name = women_summaries[i]['name']\n",
    "    end_index = name.find('(')\n",
    "    if end_index != -1 and name[: (end_index-1)] not in women_names :\n",
    "        women_names.append(name[: (end_index-1)])\n",
    "    elif end_index == -1 and name not in women_names :\n",
    "        women_names.append(name)\n",
    "num_women = len(women_names)\n",
    "# print(women_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning an index for each woman:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LATER for optimization: Assigns an index for each woman without needing a new list \n",
    "\n",
    "# # PEGAH\n",
    "# # Here, we will assign an index for each woman. This index will help us access data in numpy matrices.\n",
    "# # woman_to_index = movie_id_to_index\n",
    "# woman_to_index = {woman:index for index, woman in enumerate([d['name'] for d in women_summaries])}\n",
    "\n",
    "\n",
    "# # and because it might be useful...\n",
    "# woman_name_to_index = {name:woman_to_index[name] for name in [d['name'] for d in women_summaries]}\n",
    "# woman_index_to_name = {v:k for k,v in woman_name_to_index.items()}\n",
    "\n",
    "# print(\"The index of \\\"{}\\\" is {}\".format(women_summaries[7]['name'], woman_name_to_index[women_summaries[7]['name']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creates a dictionary {'name', [list of words in the summary]}\n",
    "# women_dict_1sent = dict()\n",
    "# # for i in range(len(women_summaries)):\n",
    "# #     name = women_summaries[i]['name']\n",
    "# for i in range(len(women_names)):\n",
    "#     name = women_names[i]\n",
    "#     summary = women_summaries[i]['summary']\n",
    "#     # to do: \n",
    "#     # hardcoded the words that we're looking for\n",
    "#     # change to regex later time\n",
    "#     index_was = summary.find('was ')\n",
    "#     index_is = summary.find('is ')\n",
    "#     index_currently = summary.find('currently ')\n",
    "#     if (index_was == -1 and index_is == -1):\n",
    "#         summary2 = summary[index_currently:]\n",
    "#         index_period = summary2.find('.')\n",
    "#         first_sent = summary2[:index_period]\n",
    "#     elif (index_was == -1 and index_currently == -1):\n",
    "#         summary2 = summary[index_is:]\n",
    "#         index_period = summary2.find('.')\n",
    "#         first_sent = summary2[:index_period]\n",
    "#     elif (index_is == -1 and index_currently == -1):\n",
    "#         summary2 = summary[index_was:]\n",
    "#         index_period = summary2.find('.')\n",
    "#         first_sent = summary2[:index_period]\n",
    "#     else :\n",
    "#         index_min = min(index_is, index_was)\n",
    "#         summary2 = summary[index_min:]\n",
    "#         index_period = summary2.find('.')\n",
    "#         first_sent = summary2[:index_period]\n",
    "#     # might want to change if we use ML that finds proper nouns\n",
    "#     # implement a stemmer\n",
    "#     first_sent_lower = first_sent.lower()\n",
    "#     tokenizer = TreebankWordTokenizer()\n",
    "#     first_sent_tok = tokenizer.tokenize(first_sent_lower)\n",
    "#     filtered_first_sent = [w for w in first_sent_tok if w not in stopwords.words('english')]\n",
    "#     women_dict_1sent[name] = filtered_first_sent\n",
    "    \n",
    "# print(women_dict_1sent)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feats = 5000 # QUESTION: What does this do?\n",
    "doc_by_vocab = np.empty([len(women_summaries), n_feats])\n",
    "\n",
    "def build_vectorizer(max_features, stop_words, max_df=0.5, min_df=1, norm='l2'):\n",
    "    \"\"\"Returns a TfidfVectorizer object\n",
    "    \n",
    "    Params: {max_features: Integer,\n",
    "             max_df: Float,\n",
    "             min_df: Float,\n",
    "             norm: String,\n",
    "             stop_words: String}\n",
    "    Returns: TfidfVectorizer\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return TfidfVectorizer(stop_words=stop_words, max_features=max_features, max_df=max_df, min_df=min_df, norm=norm)\n",
    "    \n",
    "tfidf_vec = build_vectorizer(n_feats, \"english\")\n",
    "doc_by_vocab = tfidf_vec.fit_transform([d['summary'] for d in women_summaries]).toarray()\n",
    "index_to_vocab = {i:v for i, v in enumerate(tfidf_vec.get_feature_names())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim(wom1, wom2, input_doc_mat, women_names):\n",
    "    \"\"\"Returns a float giving the cosine similarity of \n",
    "       the two movie transcripts.\n",
    "    \n",
    "    Params: {mov1: String,\n",
    "             mov2: String,\n",
    "             input_doc_mat: Numpy Array,\n",
    "             movie_name_to_index: Dict}\n",
    "    Returns: Float (Cosine similarity of the two movie transcripts.)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    a = input_doc_mat[women_names.index(wom1)] #this gives us the [......] freq of all words\n",
    "    b = input_doc_mat[women_names.index(wom2)] \n",
    "    return np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: Mary Anning vs. Lilias Armstrong\n",
      "======\n",
      "0.04431217025859715\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Check that get_sim returns the correct output\"\"\"\n",
    "print(\"Similarity: Mary Anning vs. Lilias Armstrong\")\n",
    "print(\"======\")\n",
    "test1 = get_sim('Lilias Armstrong', 'Mary Anning', doc_by_vocab, women_names)\n",
    "print(test1)\n",
    "assert test1 < 1 and test1 > 0\n",
    "print(\"\")\n",
    "\n",
    "# print(\"Similarity: Star Wars vs. Star Trek: Generations\")\n",
    "# print(\"======\")\n",
    "# test2 = get_sim('star wars', 'star trek: generations', doc_by_vocab, movie_name_to_index)\n",
    "# print(test2)\n",
    "# assert test2 < 0.25 and test2 > 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_woman_sims_cos(n_wom, input_doc_mat,input_get_sim_method, women_names):\n",
    "    \"\"\"Returns a movie_sims matrix of size (num_movies,num_movies) where for (i,j):\n",
    "        [i,j] should be the cosine similarity between the movie with index i and the movie with index j\n",
    "        \n",
    "    Note: What you set the diagonal to doesn't really matter, \n",
    "    but you should set it to 1 to indicate that all movies are trivially perfectly similar to themselves.\n",
    "    \n",
    "    Params: {n_mov: Integer,\n",
    "             movie_index_to_name: String,\n",
    "             input_doc_mat: Numpy Array,\n",
    "             movie_name_to_index: Dict,\n",
    "             input_get_sim_method: Function}\n",
    "    Returns: Numpy Array \n",
    "    \"\"\"\n",
    "    \n",
    "    women_sims = np.ones((n_wom, n_wom)) #instantiate array\n",
    "    for i in range(0, n_wom):\n",
    "        for j in range(0, n_wom):\n",
    "            women_sims[i,j] = input_get_sim_method(women_names[i], women_names[j], input_doc_mat, women_names)\n",
    "    return women_sims\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "woman_sims_cos = build_woman_sims_cos(num_women,doc_by_vocab, get_sim, women_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INCOMPLETE\n",
    "# # From A4\n",
    "# def index_search(query, sim_matrix, input_doc_mat):\n",
    "    \n",
    "#     tokenizer = TreebankWordTokenizer()\n",
    "#     q_tokens = tokenizer.tokenize(query)\n",
    "#     q = {} #vector of weights by term for query\n",
    "#     results = defaultdict(int) # document_id : score\n",
    "    \n",
    "#     vocab = index_to_vocab.values() # can be done globally too i guess\n",
    "    \n",
    "#     # count freq within query\n",
    "#     for tok in q_tokens:\n",
    "#         if tok in vocab:\n",
    "#             q[tok] = q_tokens.count(tok)\n",
    "    \n",
    "#     for q_i in q:\n",
    "#         if q_i in vocab:\n",
    "#             for rel_doc \n",
    "            \n",
    "    \n",
    "    \n",
    "#     # For token in query\n",
    "#     accum = 0\n",
    "#     for tok in q_tokens:\n",
    "#         if tok in vocab: # if the token is a word in our summary\n",
    "            \n",
    "#             accum += tfidf_vec[][]\n",
    "#         else: return \"NO!!!!!!!\"\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(index_search(\"was a biologist\", woman_sims_cos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## end cosine sim stuff pegah did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumWords(text): \n",
    "    wordDict = dict()\n",
    "    for word in text :\n",
    "        if word in wordDict :\n",
    "            wordDict[word] = wordDict[word] + 1\n",
    "        else :\n",
    "            wordDict[word] = 1\n",
    "    return (wordDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the global variable of what is equivalent to good_types\n",
    "unique_word_lst = list()\n",
    "for woman in women_dict_1sent :\n",
    "    summary = women_dict_1sent[woman]\n",
    "    for word in summary :\n",
    "        if word not in unique_word_lst :\n",
    "            unique_word_lst.append(word)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a np array where the rows are women according to the list of women names\n",
    "# columns are rows according to the list of unique words\n",
    "def create_word_freq_array(input_women_dict_1sent, input_women_names, input_num_women, input_unique_word_lst):\n",
    "    dict_freq = dict()\n",
    "\n",
    "    for woman in input_women_dict_1sent :\n",
    "        dict_freq[woman] = sumWords(input_women_dict_1sent[woman])\n",
    "    \n",
    "    np_with_freq = np.zeros(shape = (len(dict_freq), len(input_unique_word_lst)))\n",
    "    i = 0\n",
    "    for woman in input_women_names :\n",
    "#         print(woman)\n",
    "        if woman in dict_freq :\n",
    "            j = 0\n",
    "            for word in input_unique_word_lst :\n",
    "                if word in dict_freq[woman] :\n",
    "#                     print(j)\n",
    "                    np_with_freq[i][j] = dict_freq[woman][word]\n",
    "                j = j + 1\n",
    "            i = i + 1\n",
    "#     print(np_with_freq[0][0])\n",
    "    return np_with_freq\n",
    "#     print(input_unique_word_lst.index('chogha'))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_array = create_word_freq_array(women_dict_1sent, women_names, len(women_dict_1sent), unique_word_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The file is ordered from the homeworks that we've worked on\n",
    "# so that we can easily go back to the specific assignment if we need to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_j_sim_mat(input_num_women, input_word_freq_array, input_unique_word_lst):\n",
    "    arr = np.zeros(shape = (input_num_women, input_num_women))\n",
    "    \n",
    "    for (i, woman1) in enumerate(input_word_freq_array) :\n",
    "        for (j, woman2) in enumerate(input_word_freq_array) :\n",
    "            s1 = np.nonzero(woman1)\n",
    "            s2 = np.nonzero(woman2)\n",
    "            intersect = np.intersect1d(s1, s2)\n",
    "            union = np.union1d(np.array(s1).flatten(), np.array(s2).flatten())\n",
    "            if len(union) > 0 :\n",
    "                arr[i][j] = len(intersect)/len(union)\n",
    "                \n",
    "    np.fill_diagonal(arr, 0)\n",
    "    \n",
    "#     print(np.sum(arr[100:]))\n",
    "    return arr\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "j_sim_mat = create_j_sim_mat(len(women_names), word_freq_array, unique_word_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary of top 5 women for each woman\n",
    "def create_top_5_dict(j_sim_mat):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homework 3 looked at Levenshtein distance (probably not a good idea imo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homework 4 looked at cosine similarity, which we can consider on top of jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
