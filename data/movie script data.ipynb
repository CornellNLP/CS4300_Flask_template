{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie script parsing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa9f35a6450e44d1a2d125c3ca7e1934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#load in text\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "all_files = os.listdir(\"movie_scripts/\")\n",
    "\n",
    "scripts = []\n",
    "\n",
    "for file_name in tqdm(all_files):\n",
    "    pth = os.path.join(\"movie_scripts\",file_name)\n",
    "    with open(pth, 'r') as f:\n",
    "        scripts += [f.read()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(s) for s in scripts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script Lenghts:\n",
      "Mean:  191314.63605442178 \n",
      "Median:  200986.5 \n",
      "Max:  546429 \n",
      "Min:  53\n"
     ]
    }
   ],
   "source": [
    "max_l = max(lengths)\n",
    "min_l = min(lengths)\n",
    "mean_l = statistics.mean(lengths)\n",
    "median_l = statistics.median(lengths)\n",
    "\n",
    "print(\"Script Lenghts:\\nMean: \", mean_l,\"\\nMedian: \", median_l, \"\\nMax: \",max_l, \"\\nMin: \", min_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ElizabethHealy/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ElizabethHealy/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "#import pytest\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "    \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - '/Users/ElizabethHealy/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.7/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.7/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-32b022802cdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscripts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-32b022802cdc>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscripts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserver_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - '/Users/ElizabethHealy/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.7/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.7/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "tokenized = [word_tokenize(s) for s in scripts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['15 Minutes (Undated Draft)',\n",
       " 'Written by John Hertzfield',\n",
       " '2012 (2008-02 Second draft)',\n",
       " 'Written by Roland Emmerich,Harald Kloser']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test= \"\"\"\n",
    "15 Minutes (Undated Draft)\n",
    "Written by John Hertzfield\n",
    "\n",
    "2012 (2008-02 Second draft)\n",
    "Written by Roland Emmerich,Harald Kloser\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  336.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comedy:  394.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "file_lst = os.listdir(\"movie_scripts/genres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mystery.txt :  123.0\n",
      "family.txt :  53.0\n",
      "animation.txt :  45.0\n",
      "western.txt :  17.0\n",
      "adventure.txt :  203.0\n",
      "scifi.txt :  180.0\n",
      "short.txt :  3.0\n",
      "fantasy.txt :  130.0\n",
      "war.txt :  32.0\n",
      "thriller.txt :  406.0\n",
      "crime.txt :  231.0\n",
      "horror.txt :  158.0\n",
      "romance.txt :  211.0\n",
      "drama.txt :  652.0\n",
      "musical.txt :  27.0\n",
      "noir.txt :  4.0\n"
     ]
    }
   ],
   "source": [
    "for file in file_lst:\n",
    "    with open(os.path.join(\"movie_scripts/genres\",file), 'r') as f:\n",
    "        print(file,\": \", len(list(filter(None, f.read().split('\\n'))))/2)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting food words from movie data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f66af613fd34e32b440d513bd97cfa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1176\n"
     ]
    }
   ],
   "source": [
    "#load in text\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "all_files = []\n",
    "for file in os.listdir(\"movie_scripts/\"):\n",
    "    if file.endswith(\".txt\"):\n",
    "        all_files += [file]\n",
    "\n",
    "scripts = []\n",
    "\n",
    "for file_name in tqdm(all_files):\n",
    "    pth = os.path.join(\"movie_scripts\",file_name)\n",
    "    with open(pth, 'r') as f:\n",
    "        scripts += [f.read()]\n",
    "print(len(scripts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_names = [i[:-4] for i in all_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-apo-', '-assorted', '-dos', 'a-yeast', 'aai', 'abalone', 'abba-zaba', 'abbreviations', 'abietate', 'above']\n"
     ]
    }
   ],
   "source": [
    "#load in food words\n",
    "import csv\n",
    "import os\n",
    "food_words = []\n",
    "# all_files = os.listdir()\n",
    "# print(all_files)\n",
    "with open('food-related.csv', 'r') as f:\n",
    "    for l in  csv.reader(f, quotechar='\"', delimiter=',',\n",
    "                         quoting=csv.QUOTE_ALL, skipinitialspace=False):\n",
    "        food_words += [l[0]]\n",
    "print(food_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ElizabethHealy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ElizabethHealy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ElizabethHealy/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "#import pytest\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "    \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacket\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\n\\n\\n \\n                                       THE JACKET              \\n\\n               \\n                                       Written by \\n\\n                                     Massy Tadjedin\\n\\n               \\n                                Based on a screenplay by \\n\\n                                       Marc Rocco\\n\\n\\n               \\n                                                             April 15, 2003\\n\\n\\n\\n               \\n               A pure white screen.',\n",
       " 'Idyllic stillness.',\n",
       " 'All of it looking and\\n               feeling like the heavens are supposed to.',\n",
       " \"After some seconds of calm, water seems to mist the screen\\n               and the slight shifts to the left and then the right suggest\\n               this is a man's P.O.V.\",\n",
       " 'Then, suddenly, the white screen is\\n               tugged and we see it was a sheet covering a presumably dead\\n               man.',\n",
       " 'WILLIAM STARKS (V.O.)',\n",
       " 'I was 25 years old the first time I\\n                           died...\\n\\n               \\n\\n               INT.',\n",
       " \"HOSPITAL, KUWAIT, DAY\\n\\n\\n               \\n               One more tug on the sheet and we see, and suddenly hear, from\\n               William Starks' P.O.V.\",\n",
       " 'the CHAOS of the hospital around him\\n               as DOCTORS and NURSES tend as best as they can to the injured\\n               soldiers.',\n",
       " \"Our glimpse of STARKS reveals a red stretcher -- soaked in blood --\\n               and the severe head wound where a bullet's minced his skull.\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(script_names[1])\n",
    "test = sent_tokenize(scripts[1])\n",
    "test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the',\n",
       "  'jacket',\n",
       "  'written',\n",
       "  'by',\n",
       "  'massy',\n",
       "  'tadjedin',\n",
       "  'based',\n",
       "  'on',\n",
       "  'a',\n",
       "  'screenplay',\n",
       "  'by',\n",
       "  'marc',\n",
       "  'rocco',\n",
       "  'april',\n",
       "  '15',\n",
       "  ',',\n",
       "  '2003',\n",
       "  'a',\n",
       "  'pure',\n",
       "  'white',\n",
       "  'screen',\n",
       "  '.'],\n",
       " ['idyllic', 'stillness', '.'],\n",
       " ['all',\n",
       "  'of',\n",
       "  'it',\n",
       "  'looking',\n",
       "  'and',\n",
       "  'feeling',\n",
       "  'like',\n",
       "  'the',\n",
       "  'heavens',\n",
       "  'are',\n",
       "  'supposed',\n",
       "  'to',\n",
       "  '.'],\n",
       " ['after',\n",
       "  'some',\n",
       "  'seconds',\n",
       "  'of',\n",
       "  'calm',\n",
       "  ',',\n",
       "  'water',\n",
       "  'seems',\n",
       "  'to',\n",
       "  'mist',\n",
       "  'the',\n",
       "  'screen',\n",
       "  'and',\n",
       "  'the',\n",
       "  'slight',\n",
       "  'shifts',\n",
       "  'to',\n",
       "  'the',\n",
       "  'left',\n",
       "  'and',\n",
       "  'then',\n",
       "  'the',\n",
       "  'right',\n",
       "  'suggest',\n",
       "  'this',\n",
       "  'is',\n",
       "  'a',\n",
       "  'man',\n",
       "  \"'s\",\n",
       "  'p.o.v',\n",
       "  '.'],\n",
       " ['then',\n",
       "  ',',\n",
       "  'suddenly',\n",
       "  ',',\n",
       "  'the',\n",
       "  'white',\n",
       "  'screen',\n",
       "  'is',\n",
       "  'tugged',\n",
       "  'and',\n",
       "  'we',\n",
       "  'see',\n",
       "  'it',\n",
       "  'was',\n",
       "  'a',\n",
       "  'sheet',\n",
       "  'covering',\n",
       "  'a',\n",
       "  'presumably',\n",
       "  'dead',\n",
       "  'man',\n",
       "  '.'],\n",
       " ['william', 'starks', '(', 'v.o', '.', ')'],\n",
       " ['i',\n",
       "  'was',\n",
       "  '25',\n",
       "  'years',\n",
       "  'old',\n",
       "  'the',\n",
       "  'first',\n",
       "  'time',\n",
       "  'i',\n",
       "  'died',\n",
       "  '...',\n",
       "  'int',\n",
       "  '.'],\n",
       " ['hospital',\n",
       "  ',',\n",
       "  'kuwait',\n",
       "  ',',\n",
       "  'day',\n",
       "  'one',\n",
       "  'more',\n",
       "  'tug',\n",
       "  'on',\n",
       "  'the',\n",
       "  'sheet',\n",
       "  'and',\n",
       "  'we',\n",
       "  'see',\n",
       "  ',',\n",
       "  'and',\n",
       "  'suddenly',\n",
       "  'hear',\n",
       "  ',',\n",
       "  'from',\n",
       "  'william',\n",
       "  'starks',\n",
       "  \"'\",\n",
       "  'p.o.v',\n",
       "  '.'],\n",
       " ['the',\n",
       "  'chaos',\n",
       "  'of',\n",
       "  'the',\n",
       "  'hospital',\n",
       "  'around',\n",
       "  'him',\n",
       "  'as',\n",
       "  'doctors',\n",
       "  'and',\n",
       "  'nurses',\n",
       "  'tend',\n",
       "  'as',\n",
       "  'best',\n",
       "  'as',\n",
       "  'they',\n",
       "  'can',\n",
       "  'to',\n",
       "  'the',\n",
       "  'injured',\n",
       "  'soldiers',\n",
       "  '.'],\n",
       " ['our',\n",
       "  'glimpse',\n",
       "  'of',\n",
       "  'starks',\n",
       "  'reveals',\n",
       "  'a',\n",
       "  'red',\n",
       "  'stretcher',\n",
       "  '--',\n",
       "  'soaked',\n",
       "  'in',\n",
       "  'blood',\n",
       "  '--',\n",
       "  'and',\n",
       "  'the',\n",
       "  'severe',\n",
       "  'head',\n",
       "  'wound',\n",
       "  'where',\n",
       "  'a',\n",
       "  'bullet',\n",
       "  \"'s\",\n",
       "  'minced',\n",
       "  'his',\n",
       "  'skull',\n",
       "  '.']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_word = [word_tokenize(sent.lower()) for sent in test]\n",
    "test_word[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'jacket',\n",
       " 'written',\n",
       " 'by',\n",
       " 'massy',\n",
       " 'tadjedin',\n",
       " 'based',\n",
       " 'on',\n",
       " 'a',\n",
       " 'screenplay']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_word_combined = [j for sent in test_word for j in sent]\n",
    "test_word_combined[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5506\n",
      "['eyes', 'tags', 'born', 'old', 'born', 'vermont', 'family', 'listed', 'naval', 'figure', 'tags', 'back', 'open', 'eyes', 'filled', 'life', 'tear', 'runs', 'calling', 'code', 'blue', 'code', 'blue', 'man', 'others', 'code', 'blue', 'need', 'doctor', 'get', 'doctor', 'medical', 'rush', 'oxygen', 'eyes', 'follow', 'white', 'sheet', 'ground', 'ext', 'village', 'day', 'captain', 'medley', 'particularly', 'captain', 'medley', 'level', 'ground', 'really']\n"
     ]
    }
   ],
   "source": [
    "just_food_words = [i for i in test_word_combined if i in food_words and i not in stopwords.words('english')]\n",
    "print(len(just_food_words))\n",
    "print(just_food_words[50:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35331"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_word_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd039733feb4bea869c9ea3055d004e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#extract food words from all scripts\n",
    "food_word_dict = {}\n",
    "for i in tqdm(range(len(scripts))):\n",
    "    name = script_names[i]\n",
    "    script = scripts[i]\n",
    "    sents = sent_tokenize(script)\n",
    "    words = [word_tokenize(sent.lower()) for sent in sents]\n",
    "    word_combined = [j for sent in words for j in sent]\n",
    "    script_food_words = [i for i in word_combined if i in food_words and i not in stopwords.words('english')]\n",
    "    food_word_dict[name] = script_food_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"movie_food_words.json\", \"w\") as outfile: \n",
    "    json.dump(food_word_dict, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying with a different food words dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tutturosso Green 14.5oz. NSA Italian Diced Tomatoes', 'Tutturosso Green 14.5oz. Italian Diced Tomatoes', 'Honeysuckle White Fresh 97% Ground White Turkey', 'Honeysuckle White 97% Ground White Turkey', 'Honeysuckle Whtie 85% Ground Turkey', \"KELLOGG'S POP-TARTS WILD CHERRY 14.1OZ\", \"Kellogg's Pop-Tarts Frosted Strawberry 14.7oz\", 'KELLOGG POP-TARTS FROSTED STRAWBERRY 22OZ', 'Sunshine Cheez-It Crackers Original 3oz', 'Sunshine Cheez-It Crackers Original 3oz']\n"
     ]
    }
   ],
   "source": [
    "#load in food words p2\n",
    "import csv\n",
    "import os\n",
    "food_words2 = []\n",
    "# all_files = os.listdir()\n",
    "# print(all_files)\n",
    "firstline = True\n",
    "with open('food.csv', 'r') as f:\n",
    "    for l in  csv.reader(f,  delimiter=',', skipinitialspace=True):\n",
    "        if firstline: firstline=False; continue\n",
    "        food_words2 += [l[2]]\n",
    "print(food_words2[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0abce92d46274fe3819a37aecf04b5a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=498182.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "food_words2_tok_stem = set()\n",
    "for line in tqdm(food_words2):\n",
    "    tokens = word_tokenize(line.lower())\n",
    "    rm_stop = [word for word in tokens if word.isalpha() and word not in stopwords.words('english')]\n",
    "    stemmed = [porter.stem(word) for word in rm_stop]\n",
    "    food_words2_tok_stem.update(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20353\n"
     ]
    }
   ],
   "source": [
    "print(len(food_words2_tok_stem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kellogg', 'corn', 'pops', 'cereal']\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kellogg', 'corn', 'pop', 'cereal']\n",
      "['kellogg', \"'s\", 'corn', 'pop', 'cereal', '.75oz']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "c = [porter.stem(word) for word in b]\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2354270b6284ec4aa81bd9ec8a22099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#extract food words from all scripts\n",
    "scripts_stemmed = {}\n",
    "for i in tqdm(range(len(scripts))):\n",
    "    name = script_names[i]\n",
    "    script = scripts[i]\n",
    "    sents = sent_tokenize(script)\n",
    "    tokens = []\n",
    "    for sent in sents:\n",
    "        toks = word_tokenize(sent.lower())\n",
    "        tokens += toks\n",
    "    rm_stop = [word for word in tokens if word.isalpha() and word not in stopwords.words('english')]\n",
    "    stemmed = [porter.stem(word) for word in rm_stop]\n",
    "    food_words_script1 = [word for word in stemmed if word in food_words2_tok_stem]\n",
    "    scripts_stemmed[name]= food_words_script1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"movie_food_words2.json\", \"w\") as outfile: \n",
    "    json.dump(scripts_stemmed, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7680"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scripts_stemmed[\"17 Again\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying with recipe food words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"recipe_food_words.json\") as json_file:\n",
    "    recipe_food_words = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oz'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe_food_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3711da8cc0b4a4cbd2f2c92fe9323d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#extract food words from all scripts\n",
    "recipe_food_scripts = {}\n",
    "for i in tqdm(range(len(scripts))):\n",
    "    name = script_names[i]\n",
    "    script = scripts[i]\n",
    "    sents = sent_tokenize(script)\n",
    "    tokens = []\n",
    "    for sent in sents:\n",
    "        toks = word_tokenize(sent.lower())\n",
    "        tokens += toks\n",
    "    stemmed = [porter.stem(word) for word in tokens if word.isalpha()]\n",
    "    food_words_script1 = [word for word in stemmed if word in recipe_food_words]\n",
    "    recipe_food_scripts[name]= food_words_script1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"movie_food_words_from_recipes.json\", \"w\") as outfile: \n",
    "    json.dump(recipe_food_scripts, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4453\n",
      "5388\n"
     ]
    }
   ],
   "source": [
    "print(len(recipe_food_scripts[\"17 Again\"]))\n",
    "print(len(recipe_food_scripts[\"Jacket\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying with wordnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('cheese.n.01'),\n",
       " Synset('tall_mallow.n.01'),\n",
       " Synset('cheese.v.01'),\n",
       " Synset('cheese.v.02')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('cheese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def if_food(word):\n",
    "\n",
    "    syns = wn.synsets(str(word))\n",
    "\n",
    "    for syn in syns:\n",
    "        if 'food' in syn.lexname():\n",
    "            return 1\n",
    "    return 0\n",
    "#https://stackoverflow.com/questions/57057039/how-to-extract-all-words-in-a-noun-food-category-in-wordnet\n",
    "#https://blog.xrds.acm.org/2017/07/power-wordnet-use-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if_food('cheeseburger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996a624bd7ec4f2c8e48a9da981eff5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#extract food words from all scripts\n",
    "wordnet_food_scripts = {}\n",
    "for i in tqdm(range(len(scripts))):\n",
    "    name = script_names[i]\n",
    "    script = scripts[i]\n",
    "    sents = sent_tokenize(script)\n",
    "    tokens = []\n",
    "    for sent in sents:\n",
    "        toks = word_tokenize(sent.lower())\n",
    "        tokens += toks\n",
    "    stemmed = [word for word in tokens if word.isalpha() and if_food(word)]\n",
    "    #food_words_script1 = [word for word in stemmed if word in recipe_food_words]\n",
    "    wordnet_food_scripts[name]= stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordnet_food_scripts['17 Again'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"movie_food_words_from_wordnets.json\", \"w\") as outfile: \n",
    "    json.dump(wordnet_food_scripts, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('movie_food_words_from_wordnets.json') as f:\n",
    "    movie_food_words = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ElizabethHealy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ElizabethHealy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ElizabethHealy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import pytest\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "    \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from tqdm.notebook import tqdm\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c6abfe6176407e8c60d1c5763dacd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "movie_stemmed_food_words = {}\n",
    "for k,v in tqdm(movie_food_words.items()):\n",
    "    v2 = [porter.stem(word) for word in v]\n",
    "    movie_stemmed_food_words[k] = v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"movie_food_words_from_wordnets_stemmed.json\", \"w\") as outfile: \n",
    "    json.dump(movie_stemmed_food_words, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movie_stemmed_food_words[\"17 Again\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mullet',\n",
       " 'game',\n",
       " 'game',\n",
       " 'round',\n",
       " 'water',\n",
       " 'jacket',\n",
       " 'peel',\n",
       " 'cut',\n",
       " 'center',\n",
       " 'shake']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_stemmed_food_words[\"17 Again\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
